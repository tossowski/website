<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tim Ossowski</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to your stylesheet -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Ubuntu&display=swap" rel="stylesheet">
    <style>
        /* Add blue background to each section */
        section {
            background-color: #d9d9d9; /* Change to your preferred blue color */
            padding: 20px;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
            margin: 10px;
            border-radius: 5px;
        }

        body {
            font-family: 'Ubuntu', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
        }
        header {
            background-color: #333;
            color: white;
            padding: 10px 0;
            text-align: center;
        }
        nav ul {
            list-style: none;
            margin: 0;
            padding: 0;
        }
        nav li {
            display: inline;
            margin: 0 20px;
        }
        nav a {
            text-decoration: none;
            color: white;
            font-weight: bold;
        }

        #bio {
            padding: 30px
        }

        footer {
            text-align: center;
            padding: 20px;
            background-color: #333;
            color: white;
        }

        /* Style for the image container */
        .image-container {
            display: flex;
            justify-content: center;
            align-items: center;
            /* width: 600px;  */
            gap: 0;
        }

        /* Style for the scaled image */
        .image-container img {
            max-width: 100%;
            max-height: 100%;
            width: auto;
            height: auto;
            margin: 0;  
        }

 

        a {
            text-decoration: none; /* Remove underline */
            color: #3498db; /* Set link color to your desired color */
        }
    </style>
</head>
<body>
    <header>
        <div id="profile">
            
            <h1>Tim Ossowski</h1>
                <img src="profile.png" alt="Profile Pic" width="300" height="300">
            <div id="bio">
                <p>Hi! I'm a PhD student in the Computer Science Department at the University of Wisconsin-Madison advised by Professor Junjie Hu. My research is focused on the intersection between vision and language. In particular, I am interested in learning high quality multimodal representations and multimodal retrieval. </p>
            </div>
           
        </div>
        <nav>
            <ul>
                <li><a href="#publications">Research Publications</a></li>
                <li><a href="#projects">Personal Projects</a></li>
                <li><a href="#hobbies">Hobbies</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <section id="publications">
        <h2>Research Publications</h2>
        <section id="first-author">
            <h3>First Author</h3>
            <article class="publication">
                <a href="https://aclanthology.org/2023.findings-acl.158/"><h4>Multimodal Prompt Retrieval for Generative Visual Question Answering <i>(ACL Findings 2023)</i></h4></a>
                <div class="image-container">
                    <img src="architecture.svg" alt="Publication Image">
                </div>
                <p>We propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical Visual Question Answering (VQA) tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting</p>
                <p><strong>Authors:</strong> Tim Ossowski, Junjie Hu</p>
            </article>
        </section>

        <section id="other-publications">
            <h3>Other Publications</h3>
            <article class="publication">
                <a href="https://aclanthology.org/2022.findings-emnlp.12.pdf"><h4>Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment <i>(EMNLP Findings 2022)</i></h4></a>
                <div class="image-container">
                    <img src="walip.png" alt="Publication Image" width="600" height="300">
                </div>
                <p>We develop a novel Unsupervised Word Translation (UWT) method dubbed Word Alignment using Language-Image Pretraining (WALIP), leveraging visual observations via the shared image-text embedding space of <a href="https://openai.com/research/clip">CLIP models</a>. WALIP has a two-step procedure. First, we retrieve word pairs with high confidences of similarity, computed using our proposed image-based fingerprints, which define the initial pivot for the alignment. Second, we apply our robust Procrustes algorithm to estimate the linear mapping between two embedding spaces, which iteratively corrects and refines the estimated alignment. Our extensive experiments show that WALIP improves upon the state-of-the-art performance of bilingual word alignment for a few language pairs across different word embeddings and displays great robustness to the dissimilarity of language pairs or training corpora for two word embeddings.</p>
                <p><strong>Authors:</strong> Tuan Dinh, Jy-yong Sohn, Shashank Rajput, Tim Ossowski, Yifei Ming, Junjie Hu, Dimitris Papailiopoulos, Kangwook Lee</p>
            </article>
        </section>  
    </section>

    <section id="projects">
        <h2>Personal Projects</h2>
        <article class="publication">
            <a href="https://github.com/tossowski/CubicChunksMapViewer/tree/main"><h4>Cubic Chunks Map Viewer</i></h4></a>
            <div class="image-container">
                <img src="halfdome.png" alt="Half Dome Image" width="600" height="300">
            </div>
            <p>A program which allows any cubic chunks minecraft world to be rendered as an isometric interactive map. The picture above is generated from the halfdome region in California of the terra 1 to 1 mod. Working on a version which allows any 3d model to be converted to a map. </p>
        </article>

        <article class="publication">
            <a href="https://tossowski.github.io/VoxelDex/"><h4>VoxelDex</i></h4></a>
            <div class="image-container">
                <img src="bulbasaur.png" alt="Bulbasaur Image" width="600" height="300">
            </div>
            <p>Using the cubic chunks map viewer, I rendered a collection of all generation 1 pokemon. More details on the project page</p>
        </article>
        <!-- Add your personal project content here -->
    </section>

    <section id="hobbies">
        <h2>Hobbies</h2>
        <!-- Add your hobbies content here -->
    </section>

    <section id="contact">
        <h2>Contact Me</h2>
        <p>If you have any questions or would like to connect, feel free to reach me at:</p>
        <p>Email: <a href="mailto:ossowski@wisc.edu">ossowski@wisc.edu</a></p>
        <p>LinkedIn: <a href="https://www.linkedin.com/in/tim-ossowski-bb07a0171/" target="_blank" rel="noopener noreferrer">Tim Ossowski</a></p>
    </section>

    <footer>
        <p>&copy; 2023 Tim Ossowski. All rights reserved.</p>
    </footer>
</body>
</html>
